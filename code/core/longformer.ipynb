{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import LongformerForSequenceClassification, LongformerTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_gpu = torch.cuda.device_count()\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"E:/personal/Code/Python/LegalRetrieval/data/longformer_out/\"\n",
    "data_info_file = \"E:/personal/Code/Python/LegalRetrieval/data/full_data_labels.json\"\n",
    "\n",
    "list_skipped_words = ['the', 'a', 'an', 'in', 'on', 'at', 'to', 'of', 'for', 'with', 'by', 'and', 'or', 'but', 'so', 'nor', 'yet', 'from', 'into', 'onto', 'upon', 'out', 'off', 'over', 'under', 'below', 'above', 'between', 'among', 'through', 'during', 'before', 'after', 'since', 'until', 'while', 'as', 'like', 'about', 'against', 'among', 'around', 'before', 'behind', 'beneath', 'beside', 'between', 'beyond', 'during', 'inside', 'outside', 'underneath', 'within', 'without', 'throughout', 'along', 'across', 'toward', 'towards', 'up', 'down', 'forward', 'backward', 'right', 'left', 'here', 'there', 'where', 'when', 'why', 'how', 'what', 'which', 'who', 'whom', 'whose', 'whichever', 'whatever', 'whomever', 'whenever', 'wherever', 'however', 'whyever', ',', ';']\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = text.split()\n",
    "    filtered_text = [word for word in word_tokens if word not in list_skipped_words]\n",
    "    # return a string\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "case_law = {}\n",
    "for file in os.listdir(data_folder):\n",
    "    with open(os.path.join(data_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        case_id = file.split(\".\")[0]\n",
    "        full_text = data[\"meta\"] + ' '\n",
    "        for par in data[\"paragraphs\"]:\n",
    "            full_text += par + ' '\n",
    "        full_text = remove_stopwords(full_text)\n",
    "        case_law[case_id] = full_text\n",
    "\n",
    "data_info = json.load(open(data_info_file, \"r\"))\n",
    "pairs = []\n",
    "for query in data_info:\n",
    "    query_text = case_law[query]\n",
    "    candidates = data_info[query]\n",
    "    for candidate in candidates:\n",
    "        candidate_text = case_law[candidate]\n",
    "        pairs.append({\"text\": [query_text, candidate_text], \"label\": candidates[candidate]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = max_length\n",
    "        self.num_pairs = len(pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_pairs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # combine the query and candidate text in pairs[idx][\"text\"]\n",
    "        text = self.pairs[idx][\"text\"]\n",
    "        label = self.pairs[idx][\"label\"]\n",
    "        # encode the text, text is the list of query and candidate text\n",
    "        encoding = self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=self.seq_len, return_tensors=\"pt\")\n",
    "        # return the encoded text and the label\n",
    "        return {\"input_ids\": encoding[\"input_ids\"].squeeze(), \"attention_mask\": encoding[\"attention_mask\"].squeeze(), \"labels\": torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate data into train and test\n",
    "train_pairs, test_pairs = train_test_split(pairs, test_size=0.2)\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "max_length = 4096\n",
    "train_dataset = CustomDataset(train_pairs, tokenizer, max_length)\n",
    "test_dataset = CustomDataset(test_pairs, tokenizer, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train_dataset[random.randrange(len(train_dataset))]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Loss: {total_loss/len(train_loader)}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
